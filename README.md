# VLLM + Weaviate Integration Repo

This repository demonstrates how to build an interactive application leveraging a **VLLM server** for hosting a language model combined with **Weaviate** for document embedding and semantic search.

---

## Features

- **VLLM Server:** Host and serve a powerful language model using VLLM.
- **Weaviate Client:** Embed and index documents for efficient semantic retrieval.
- **Streamlit Application:** User-friendly web interface to upload documents and interact with the language model.
- **Console Application:** CLI version of the app for quick interactions without a GUI.
- **Document Upload:** Easily upload various documents to enrich the knowledge base.

---

## Getting Started

### Prerequisites

Before running any code, ensure you have the VLLM environment set up properly. VLLM requires GPU support and specific configurations.

### VLLM Setup Guide

Follow the official installation guide to set up VLLM with GPU support:

[https://docs.vllm.ai/en/latest/getting_started/installation/gpu.html#amd-rocm](https://docs.vllm.ai/en/latest/getting_started/installation/gpu.html#amd-rocm)

